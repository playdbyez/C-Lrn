 ############################### :: POSIX THREADS :: ################################

<> This method of writing parallel programs relies on the POSIX standard of threads, supported by many operatings systems.  
   Defined within the <pthread.h> library and with its own supporting data-type 'pthread_t' for storage, this POSIX 
   implementation is referred to as pthread

<> Threads is designed wih parallel programing in mind, for the benefit of making a program
   execute faster via a multi-core processor.

<> A program is defined by a main thread regardless if the pthread function has been called, 
   hence is usually neccessary to use thread_join after thread_create in order to sync the newly created thread.

<> In regards to core utilization, programs can then be divided into two types:
   
   > Sequential Program = Single thread process designed for a single core system
      > Operations : 
                    - Single Instruction on Single Data (SISD) 
                    - Operating System Multitasking


   > Parallel  Program  = A two or more thread process designed for a multi-core system
      > Operations : 
                    - Single Instruction on Multiple Data (SIMD) 
                    - Multiple Instructions on Single Data (MISD)
                    - Multiple Instructions on Multiple Data (MIMD)

<> Parallel programs can also run on single core processors via Multitasking which is
   supported by the OS in simulating a multi-core environment.
   This is achieved by giving each program a short time interval to make progress before being suspended in 
   order to allow another time interval for anoter program to make progress.
   To maximize performance, these intervals' length may vary depending on the function being executed
   and these variations however to the detriment of tracking its execution, makes it hard to
   predict exactly which program is running at any specific moment.

<> When compiling a pthread program its necessary to link it to the pthread library in order 
   to enable core management within gcc with the following argument:
      $ gcc main.c -lpthread

<> Pthread macro:
#include <pthread.h>
int pthread_create (  pthread_t * restrict thread,
                    const pthread_attr_t * restrict attr,
                    void * ( *START_ROUTINE) (void*),
                    void * restrict arg )


  1) address of a pthread_t object.

  2) address of a pthread_attr_t structure can be used to define optional 
     features to the thread being created, use NULL for creating a thread with default options.

  3) void * type function to be executed within this thread's life cycle, the functions must have 
     only one argument and it must be of type void 
  4) The argument to the fuctions specified in the previous section, it must be cast to void * 





<> Pthread function calling:
main ()
{ pthread_t st;
  int arg = 12;
  int rtv;
  rtv = pthread_create(&st, NULL, printHello, (void*) &arg);
          if (rtv != 0) { printf("ERROR: Thread creation failed.\n"); }
  rtv = pthread_join(second, NULL)
          if (rtv != 0) { printf("ERROR: Thread synchronization failed.\n"); }

}


############################### :: AMDAHL'S LAW :: ################################
>> CHAPTER 23 <<

Generally a program's execution time can be improved either through overclocking or threading.
For a parallel program, when pipelining is applied efficiently in order to synchronize a threaded program's multiple operations into a single output, the concept of "diminishing return" is then also applied to its execution speed and this entails reducing its execution time through increasing the amount of cores used by the program.
 
However, a certain limitation imposed by a programs' initialization and finalization stages will always account for 1% of its total execution time.
Thus, the increase of cores will only account for optimizing the remaining 99% of the execution time.
 
Example program:
> TotalExecTime  (T) 
> Init/Fin       (R)
> PipelinedExec  (Pe)
> N# Cores       (n)
 
Calculation of execution improvement per n cores:
T  = 100s
R  = 1s
Pe = 99s
n  = 2
Pe2 = Pe / n  = 49.5s
T   = R + Pe2 = 50.5  
>> Improvement of 49.5s shaved off its previous time <<


Additionally, Amdahl's law  infers that another limitation can be applied to parallel programs which refers to 
the variable percentage of execution that is not amenable to pipelining (r).

These limitations can be identified more accurately through a calculus approach via Amdahl's "Speedup formula" that 
makes use of the "Efficiency formula" noted bellow.

 ...

Although Amdahl's law asserts that there is always a serial part to a parallel program that must be executed sequentially.
There are also flaws in ahmdahl's law as argued by Gustafson & Barsis: 
dl.acm.org/doi/10.1145/42411.42415
 
 
 


trueleappress.com/wp-content/uploads/2023/03/coping-with-snitch-culture-historical-examples-current-proposals.pdf
 
 
https://www.youtube.com/watch?v=LG-liWi-04w
https://www.youtube.com/watch?v=DeI8cCX9o4I


https://www.youtube.com/watch?v=Me96OWd44q0




https://www.youtube.com/watch?v=LOfGJcVnvAk
https://www.youtube.com/watch?v=4rLW7zg21gI
https://www.youtube.com/watch?v=uA8X5zNOGw8
https://www.youtube.com/watch?v=ukM_zzrIeXs
https://www.youtube.com/watch?v=oq29KUy29iQ
https://www.youtube.com/watch?v=_n2hE2gyPxU
https://www.youtube.com/watch?v=Pg_4Jz8ZIH4
https://www.youtube.com/watch?v=q24-QTbKQS8
https://www.youtube.com/watch?v=9axu8CUvOKY
